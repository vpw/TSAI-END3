{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vpw/TSAI-END3/blob/main/Assignment5/Assignment5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxpGkSrs4ydd"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import YelpReviewPolarity, SogouNews\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO8QE0EXoBsb"
      },
      "source": [
        "https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Cr9QuiQ5cge",
        "outputId": "4db85490-82b8-43f6-87ab-9921371cd5ed"
      },
      "source": [
        "help(SogouNews)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function SogouNews in module torchtext.datasets.sogounews:\n",
            "\n",
            "SogouNews(root='.data', split=('train', 'test'))\n",
            "    SogouNews dataset\n",
            "    \n",
            "    Separately returns the train/test split\n",
            "    \n",
            "    Number of lines per split:\n",
            "        train: 450000\n",
            "    \n",
            "        test: 60000\n",
            "    \n",
            "    \n",
            "    Number of classes\n",
            "        5\n",
            "    \n",
            "    \n",
            "    Args:\n",
            "        root: Directory where the datasets are saved.\n",
            "            Default: .data\n",
            "        split: split or splits to be returned. Can be a string or tuple of strings.\n",
            "            Default: ('train', 'test')\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCr_GoKsvXim"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbWGBHqcjD0Q"
      },
      "source": [
        "## Model as given in the assignment code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZdDyqjm88gS"
      },
      "source": [
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoOjLSGe5Cg7"
      },
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR690WyACyma"
      },
      "source": [
        "## Making a TrainTestDataset class which takes as parameters: dataset, #epochs to train, LR, batch size to use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsCThGaB-EOD"
      },
      "source": [
        "class TrainTestDataset:\n",
        "    def __init__(self, dataset, epochs, lr, batch_size):\n",
        "        self.dataset = dataset\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "\n",
        "        train_iter = dataset(split='train')\n",
        "        # Buid vocabulary\n",
        "        self.vocab = build_vocab_from_iterator(self.yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "        self.vocab.set_default_index(self.vocab[\"<unk>\"])\n",
        "        # Text and label pipeline\n",
        "        self.text_pipeline = lambda x: self.vocab(tokenizer(x))\n",
        "        self.label_pipeline = lambda x: int(x) - 1\n",
        "\n",
        "        # random sample text is selected for a sample demo at the end\n",
        "        self.sample_text=\"\"\n",
        "\n",
        "    # function as in assignment source\n",
        "    def yield_tokens(self, data_iter):\n",
        "        for _, text in data_iter:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "    # collate function as in assignment source\n",
        "    def collate_batch(self, batch):\n",
        "        label_list, text_list, offsets = [], [], [0]\n",
        "        for (_label, _text) in batch:\n",
        "            label_list.append(self.label_pipeline(_label))\n",
        "            processed_text = torch.tensor(self.text_pipeline(_text), dtype=torch.int64)\n",
        "            text_list.append(processed_text)\n",
        "            offsets.append(processed_text.size(0))\n",
        "        label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "        offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "        text_list = torch.cat(text_list)\n",
        "        return label_list.to(device), text_list.to(device), offsets.to(device)    \n",
        "\n",
        "    # added parameters - criterion, optimizer and epoch - to be called in the train and test function loop\n",
        "    def train(self, dataloader, criterion, optimizer, epoch):\n",
        "        self.model.train()\n",
        "        total_acc, total_count = 0, 0\n",
        "        log_interval = 500\n",
        "        start_time = time.time()\n",
        "\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            predited_label = self.model(text, offsets)\n",
        "            loss = criterion(predited_label, label)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.1) # disuccees\n",
        "            optimizer.step()\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "            if idx % log_interval == 0 and idx > 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                      '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                                  total_acc/total_count))\n",
        "                total_acc, total_count = 0, 0\n",
        "                start_time = time.time()\n",
        "    \n",
        "    # added parameter - criterion\n",
        "    def evaluate(self, dataloader, criterion):\n",
        "        self.model.eval()\n",
        "        total_acc, total_count = 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "                predited_label = self.model(text, offsets)\n",
        "                loss = criterion(predited_label, label)\n",
        "                total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "                total_count += label.size(0)\n",
        "        return total_acc/total_count\n",
        "\n",
        "    def train_and_test(self):\n",
        "        \n",
        "        train_iter = self.dataset(split = 'train')\n",
        "        num_class = len(set([label for (label, text) in train_iter]))\n",
        "        vocab_size = len(self.vocab)\n",
        "        emsize = 64\n",
        "        self.model = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n",
        "        \n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "        total_accu = None\n",
        "        train_iter, test_iter = self.dataset()\n",
        "        \n",
        "        train_dataset = to_map_style_dataset(train_iter)\n",
        "        test_dataset = to_map_style_dataset(test_iter)\n",
        "\n",
        "        self.sample_text = test_dataset[random.randint(0,len(test_dataset))]\n",
        "\n",
        "        num_train = int(len(train_dataset) * 0.95)\n",
        "        split_train_, split_valid_ = \\\n",
        "            random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "        #dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
        "        train_dataloader = DataLoader(split_train_, batch_size=self.batch_size,\n",
        "                                      shuffle=True, collate_fn=self.collate_batch)\n",
        "        valid_dataloader = DataLoader(split_valid_, batch_size=self.batch_size,\n",
        "                                      shuffle=True, collate_fn=self.collate_batch)\n",
        "        test_dataloader = DataLoader(test_dataset, batch_size=self.batch_size,\n",
        "                                    shuffle=True, collate_fn=self.collate_batch)\n",
        "\n",
        "        for epoch in range(1, self.epochs + 1):\n",
        "            epoch_start_time = time.time()\n",
        "            self.train(train_dataloader, criterion, optimizer, epoch)\n",
        "            accu_val = self.evaluate(valid_dataloader, criterion)\n",
        "            if total_accu is not None and total_accu > accu_val:\n",
        "              scheduler.step()\n",
        "            else:\n",
        "              total_accu = accu_val\n",
        "            print('-' * 59)\n",
        "            print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "                  'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                                  time.time() - epoch_start_time,\n",
        "                                                  accu_val))\n",
        "            print('-' * 59)\n",
        "\n",
        " \n",
        "        print('Checking the results of test dataset.')\n",
        "        accu_test = self.evaluate(test_dataloader, criterion)\n",
        "        print('test accuracy {:8.3f}'.format(accu_test))\n",
        "\n",
        "    def predict(self, text, text_pipeline):\n",
        "        with torch.no_grad():\n",
        "            text = torch.tensor(text_pipeline(text))\n",
        "            output = self.model(text, torch.tensor([0]))\n",
        "            return output.argmax(1).item() + 1\n",
        "    \n",
        "    def test_sample(self, labels):\n",
        "        self.model = self.model.to(\"cpu\")\n",
        "        print(self.sample_text)\n",
        "\n",
        "        print(\"This is a %s news\" %labels[self.predict(self.sample_text, self.text_pipeline)])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2UT4qjhnEiY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS2oekIz_Gqu",
        "outputId": "a3608bde-c11a-41c7-ae67-0d044fd27c37"
      },
      "source": [
        "tt = TrainTestDataset(YelpReviewPolarity, epochs=5, lr=5, batch_size=64)\n",
        "tt.train_and_test()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/ 8313 batches | accuracy    0.783\n",
            "| epoch   1 |  1000/ 8313 batches | accuracy    0.863\n",
            "| epoch   1 |  1500/ 8313 batches | accuracy    0.880\n",
            "| epoch   1 |  2000/ 8313 batches | accuracy    0.887\n",
            "| epoch   1 |  2500/ 8313 batches | accuracy    0.892\n",
            "| epoch   1 |  3000/ 8313 batches | accuracy    0.897\n",
            "| epoch   1 |  3500/ 8313 batches | accuracy    0.899\n",
            "| epoch   1 |  4000/ 8313 batches | accuracy    0.900\n",
            "| epoch   1 |  4500/ 8313 batches | accuracy    0.906\n",
            "| epoch   1 |  5000/ 8313 batches | accuracy    0.905\n",
            "| epoch   1 |  5500/ 8313 batches | accuracy    0.903\n",
            "| epoch   1 |  6000/ 8313 batches | accuracy    0.910\n",
            "| epoch   1 |  6500/ 8313 batches | accuracy    0.908\n",
            "| epoch   1 |  7000/ 8313 batches | accuracy    0.907\n",
            "| epoch   1 |  7500/ 8313 batches | accuracy    0.913\n",
            "| epoch   1 |  8000/ 8313 batches | accuracy    0.911\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 80.01s | valid accuracy    0.908 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 8313 batches | accuracy    0.917\n",
            "| epoch   2 |  1000/ 8313 batches | accuracy    0.913\n",
            "| epoch   2 |  1500/ 8313 batches | accuracy    0.914\n",
            "| epoch   2 |  2000/ 8313 batches | accuracy    0.914\n",
            "| epoch   2 |  2500/ 8313 batches | accuracy    0.916\n",
            "| epoch   2 |  3000/ 8313 batches | accuracy    0.918\n",
            "| epoch   2 |  3500/ 8313 batches | accuracy    0.916\n",
            "| epoch   2 |  4000/ 8313 batches | accuracy    0.917\n",
            "| epoch   2 |  4500/ 8313 batches | accuracy    0.918\n",
            "| epoch   2 |  5000/ 8313 batches | accuracy    0.919\n",
            "| epoch   2 |  5500/ 8313 batches | accuracy    0.916\n",
            "| epoch   2 |  6000/ 8313 batches | accuracy    0.919\n",
            "| epoch   2 |  6500/ 8313 batches | accuracy    0.919\n",
            "| epoch   2 |  7000/ 8313 batches | accuracy    0.921\n",
            "| epoch   2 |  7500/ 8313 batches | accuracy    0.917\n",
            "| epoch   2 |  8000/ 8313 batches | accuracy    0.920\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 80.42s | valid accuracy    0.912 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 8313 batches | accuracy    0.921\n",
            "| epoch   3 |  1000/ 8313 batches | accuracy    0.923\n",
            "| epoch   3 |  1500/ 8313 batches | accuracy    0.922\n",
            "| epoch   3 |  2000/ 8313 batches | accuracy    0.921\n",
            "| epoch   3 |  2500/ 8313 batches | accuracy    0.922\n",
            "| epoch   3 |  3000/ 8313 batches | accuracy    0.921\n",
            "| epoch   3 |  3500/ 8313 batches | accuracy    0.923\n",
            "| epoch   3 |  4000/ 8313 batches | accuracy    0.925\n",
            "| epoch   3 |  4500/ 8313 batches | accuracy    0.922\n",
            "| epoch   3 |  5000/ 8313 batches | accuracy    0.924\n",
            "| epoch   3 |  5500/ 8313 batches | accuracy    0.925\n",
            "| epoch   3 |  6000/ 8313 batches | accuracy    0.924\n",
            "| epoch   3 |  6500/ 8313 batches | accuracy    0.924\n",
            "| epoch   3 |  7000/ 8313 batches | accuracy    0.922\n",
            "| epoch   3 |  7500/ 8313 batches | accuracy    0.922\n",
            "| epoch   3 |  8000/ 8313 batches | accuracy    0.923\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 80.57s | valid accuracy    0.918 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 8313 batches | accuracy    0.925\n",
            "| epoch   4 |  1000/ 8313 batches | accuracy    0.925\n",
            "| epoch   4 |  1500/ 8313 batches | accuracy    0.926\n",
            "| epoch   4 |  2000/ 8313 batches | accuracy    0.927\n",
            "| epoch   4 |  2500/ 8313 batches | accuracy    0.923\n",
            "| epoch   4 |  3000/ 8313 batches | accuracy    0.925\n",
            "| epoch   4 |  3500/ 8313 batches | accuracy    0.924\n",
            "| epoch   4 |  4000/ 8313 batches | accuracy    0.924\n",
            "| epoch   4 |  4500/ 8313 batches | accuracy    0.924\n",
            "| epoch   4 |  5000/ 8313 batches | accuracy    0.927\n",
            "| epoch   4 |  5500/ 8313 batches | accuracy    0.927\n",
            "| epoch   4 |  6000/ 8313 batches | accuracy    0.930\n",
            "| epoch   4 |  6500/ 8313 batches | accuracy    0.927\n",
            "| epoch   4 |  7000/ 8313 batches | accuracy    0.927\n",
            "| epoch   4 |  7500/ 8313 batches | accuracy    0.928\n",
            "| epoch   4 |  8000/ 8313 batches | accuracy    0.925\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 80.62s | valid accuracy    0.921 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 8313 batches | accuracy    0.927\n",
            "| epoch   5 |  1000/ 8313 batches | accuracy    0.926\n",
            "| epoch   5 |  1500/ 8313 batches | accuracy    0.928\n",
            "| epoch   5 |  2000/ 8313 batches | accuracy    0.926\n",
            "| epoch   5 |  2500/ 8313 batches | accuracy    0.930\n",
            "| epoch   5 |  3000/ 8313 batches | accuracy    0.928\n",
            "| epoch   5 |  3500/ 8313 batches | accuracy    0.930\n",
            "| epoch   5 |  4000/ 8313 batches | accuracy    0.927\n",
            "| epoch   5 |  4500/ 8313 batches | accuracy    0.928\n",
            "| epoch   5 |  5000/ 8313 batches | accuracy    0.928\n",
            "| epoch   5 |  5500/ 8313 batches | accuracy    0.927\n",
            "| epoch   5 |  6000/ 8313 batches | accuracy    0.927\n",
            "| epoch   5 |  6500/ 8313 batches | accuracy    0.931\n",
            "| epoch   5 |  7000/ 8313 batches | accuracy    0.927\n",
            "| epoch   5 |  7500/ 8313 batches | accuracy    0.929\n",
            "| epoch   5 |  8000/ 8313 batches | accuracy    0.929\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 80.47s | valid accuracy    0.893 \n",
            "-----------------------------------------------------------\n",
            "Checking the results of test dataset.\n",
            "test accuracy    0.897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3mdAcNBDn3g"
      },
      "source": [
        "tt.test_sample(\n",
        "    labels={0: \"Negative polarity\", 1: \"Positive polarity\"}\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08QgUxA1bkTk",
        "outputId": "b36b5c1d-4f85-451c-99c1-cef631026bd0"
      },
      "source": [
        "tt = TrainTestDataset(SogouNews, epochs=5, lr=5, batch_size=64)\n",
        "tt.train_and_test()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 384M/384M [00:03<00:00, 111MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/ 6680 batches | accuracy    0.812\n",
            "| epoch   1 |  1000/ 6680 batches | accuracy    0.907\n",
            "| epoch   1 |  1500/ 6680 batches | accuracy    0.919\n",
            "| epoch   1 |  2000/ 6680 batches | accuracy    0.919\n",
            "| epoch   1 |  2500/ 6680 batches | accuracy    0.924\n",
            "| epoch   1 |  3000/ 6680 batches | accuracy    0.923\n",
            "| epoch   1 |  3500/ 6680 batches | accuracy    0.925\n",
            "| epoch   1 |  4000/ 6680 batches | accuracy    0.927\n",
            "| epoch   1 |  4500/ 6680 batches | accuracy    0.928\n",
            "| epoch   1 |  5000/ 6680 batches | accuracy    0.927\n",
            "| epoch   1 |  5500/ 6680 batches | accuracy    0.930\n",
            "| epoch   1 |  6000/ 6680 batches | accuracy    0.925\n",
            "| epoch   1 |  6500/ 6680 batches | accuracy    0.929\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 174.42s | valid accuracy    0.929 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 6680 batches | accuracy    0.931\n",
            "| epoch   2 |  1000/ 6680 batches | accuracy    0.931\n",
            "| epoch   2 |  1500/ 6680 batches | accuracy    0.932\n",
            "| epoch   2 |  2000/ 6680 batches | accuracy    0.931\n",
            "| epoch   2 |  2500/ 6680 batches | accuracy    0.930\n",
            "| epoch   2 |  3000/ 6680 batches | accuracy    0.932\n",
            "| epoch   2 |  3500/ 6680 batches | accuracy    0.933\n",
            "| epoch   2 |  4000/ 6680 batches | accuracy    0.932\n",
            "| epoch   2 |  4500/ 6680 batches | accuracy    0.932\n",
            "| epoch   2 |  5000/ 6680 batches | accuracy    0.930\n",
            "| epoch   2 |  5500/ 6680 batches | accuracy    0.932\n",
            "| epoch   2 |  6000/ 6680 batches | accuracy    0.933\n",
            "| epoch   2 |  6500/ 6680 batches | accuracy    0.932\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 175.18s | valid accuracy    0.932 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 6680 batches | accuracy    0.930\n",
            "| epoch   3 |  1000/ 6680 batches | accuracy    0.933\n",
            "| epoch   3 |  1500/ 6680 batches | accuracy    0.934\n",
            "| epoch   3 |  2000/ 6680 batches | accuracy    0.934\n",
            "| epoch   3 |  2500/ 6680 batches | accuracy    0.935\n",
            "| epoch   3 |  3000/ 6680 batches | accuracy    0.932\n",
            "| epoch   3 |  3500/ 6680 batches | accuracy    0.937\n",
            "| epoch   3 |  4000/ 6680 batches | accuracy    0.935\n",
            "| epoch   3 |  4500/ 6680 batches | accuracy    0.933\n",
            "| epoch   3 |  5000/ 6680 batches | accuracy    0.937\n",
            "| epoch   3 |  5500/ 6680 batches | accuracy    0.935\n",
            "| epoch   3 |  6000/ 6680 batches | accuracy    0.931\n",
            "| epoch   3 |  6500/ 6680 batches | accuracy    0.936\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 176.47s | valid accuracy    0.931 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 6680 batches | accuracy    0.940\n",
            "| epoch   4 |  1000/ 6680 batches | accuracy    0.939\n",
            "| epoch   4 |  1500/ 6680 batches | accuracy    0.939\n",
            "| epoch   4 |  2000/ 6680 batches | accuracy    0.939\n",
            "| epoch   4 |  2500/ 6680 batches | accuracy    0.937\n",
            "| epoch   4 |  3000/ 6680 batches | accuracy    0.938\n",
            "| epoch   4 |  3500/ 6680 batches | accuracy    0.939\n",
            "| epoch   4 |  4000/ 6680 batches | accuracy    0.940\n",
            "| epoch   4 |  4500/ 6680 batches | accuracy    0.940\n",
            "| epoch   4 |  5000/ 6680 batches | accuracy    0.940\n",
            "| epoch   4 |  5500/ 6680 batches | accuracy    0.938\n",
            "| epoch   4 |  6000/ 6680 batches | accuracy    0.938\n",
            "| epoch   4 |  6500/ 6680 batches | accuracy    0.939\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 174.09s | valid accuracy    0.936 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 6680 batches | accuracy    0.940\n",
            "| epoch   5 |  1000/ 6680 batches | accuracy    0.939\n",
            "| epoch   5 |  1500/ 6680 batches | accuracy    0.944\n",
            "| epoch   5 |  2000/ 6680 batches | accuracy    0.940\n",
            "| epoch   5 |  2500/ 6680 batches | accuracy    0.940\n",
            "| epoch   5 |  3000/ 6680 batches | accuracy    0.938\n",
            "| epoch   5 |  3500/ 6680 batches | accuracy    0.940\n",
            "| epoch   5 |  4000/ 6680 batches | accuracy    0.939\n",
            "| epoch   5 |  4500/ 6680 batches | accuracy    0.939\n",
            "| epoch   5 |  5000/ 6680 batches | accuracy    0.938\n",
            "| epoch   5 |  5500/ 6680 batches | accuracy    0.939\n",
            "| epoch   5 |  6000/ 6680 batches | accuracy    0.937\n",
            "| epoch   5 |  6500/ 6680 batches | accuracy    0.939\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 179.07s | valid accuracy    0.936 \n",
            "-----------------------------------------------------------\n",
            "Checking the results of test dataset.\n",
            "test accuracy    0.935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "FY5u2qUvbpyl",
        "outputId": "76ac0b38-dd3a-45ab-a26c-5c611595c338"
      },
      "source": [
        "tt.test_sample(\n",
        "    labels={0: 'Sports',\n",
        "                  1: 'Finance',\n",
        "                  2: 'Entertainment',\n",
        "                  3: 'Automobile',\n",
        "                  4: 'Technology'}\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, ' duo1 ca3i   be3n be3n rui4 ya3n DLV-B57 cha3n pi3n mi2ng che1ng \\\\n  duo1 ca3i   be3n be3n rui4 ya3n DLV-B57\\\\n  jia4 ge2 : ￥99\\\\n  ji1 be3n ca1n shu4   duo1 ca3i   be3n be3n rui4 ya3n DLV-B57\\\\n  shi4 fo3u fa2ng da4o   fo3u \\\\n  shi4 yo4ng le4i xi2ng   bi3 ji4 be3n \\\\n  qu1 do4ng le4i xi2ng   wu2 \\\\n  xi4 to3ng zhi1 chi2  Win 2000/XP/ Vista de3ng ca1o zuo4 xi4 to3ng \\\\n  shi4 pi2n tu2 xia4ng   ge2 shi4 :YUY2\\\\n  jie1 ko3u le4i xi2ng  USB2.0, xia4ng xia4 jia1n ro2ng USB1.1\\\\n  zui4 da4 zhe1n pi2n  30\\\\n  she4 xia4ng to2u xia4ng su4  130 wa4n \\\\n  che2ng xia4ng ju4 li2  3CM-- wu2 xia4n yua3n \\\\n  xi4n za4o bi3   yo1u yu2 48\\\\n  yua2n jia4n mia2o shu4   xi1n yi1 da4i USB2.0 te4 xia4o duo1 go1ng ne2ng zhu3 ko4ng xi1n pia4n \\\\n  ga3n gua1ng yua2n jia4n  CMOS\\\\n  do4ng ta4i fe1n bia4n shua4i  1280×960\\\\n  xi4ng ne2ng ca1n shu4   duo1 ca3i   be3n be3n rui4 ya3n DLV-B57\\\\n  zui4 xia3o li2ng mi3n du4  1.0V/Lux-sec\\\\n  se4 ca3i we4i shu4  24 we4i \\\\n  shi4 cha2ng  ≥ 53 du4 \\\\n  ba4o gua1ng ko4ng zhi4   zi4 do4ng ,  sho3u do4ng \\\\n  ba2i pi2ng he2ng   zi4 do4ng ,  sho3u do4ng \\\\n  ji4ng to2u mia2o shu4   bo1 li2 ji4ng to2u \\\\n  qi2 ta1 ca1n shu4   duo1 ca3i   be3n be3n rui4 ya3n DLV-B57\\\\n  qi2 ta1 xi4ng ne2ng   yu4 zhi4 duo1 zho3ng te4 xia4o go1ng ne2ng  , qu4 we4i xia1ng kua4ng ke3 go1ng xua3n ze2 lia3n bu4 shi2 bie2 zhui1 zo1ng xi4 to3ng , shu4 ma3 wu2 ji2 bia4n jia1o ,  zhi1 chi2 yi4ng jia4n kua4i pa1i \\\\n  qi2 ta1 te4 dia3n   su4 du4 :800*600 15 zhe1n / mia3o (USB2.0 jie1 ko3u ) \\\\n  cha3n pi3n ya2n se4   he1i se4 \\\\n  wa4i xi2ng she4 ji4   ji1ng zhi4 xia3o qia3o , bi3 ji4 be3n dia4n na3o yo4ng hu4 di2 sho3u xua3n hui1 se4 \\\\n  sui2 ji1 fu4 jia4n   duo1 ca3i   be3n be3n rui4 ya3n DLV-B57\\\\n  fu4 da4i rua3n jia4n   cha3n pi3n shuo1 mi2ng shu1 , a1n zhua1ng gua1ng pa2n ')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-018a8b7eeaf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Entertainment'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Automobile'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                   4: 'Technology'}\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-5-b8f99a5db648>\u001b[0m in \u001b[0;36mtest_sample\u001b[0;34m(self, labels)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This is a %s news\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_pipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-b8f99a5db648>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, text, text_pipeline)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-b8f99a5db648>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<unk>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Text and label pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/data/utils.py\u001b[0m in \u001b[0;36m_basic_english_normalize\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpattern_re\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplaced_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_patterns_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplaced_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_uFkXFFb-t7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}