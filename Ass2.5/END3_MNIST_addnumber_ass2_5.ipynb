{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "END3-MNIST-addnumber-ass2.5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vpw/TSAI-END3/blob/main/Ass2.5/END3_MNIST_addnumber_ass2_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poqOrQo5F3Hj"
      },
      "source": [
        "# Quick Start PyTorch - MNIST\n",
        "\n",
        "To run a Code Cell you can click on the `‚èØ Run` button in the Navigation Bar above or type `Shift + Enter`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEAqUCX13sKj"
      },
      "source": [
        "#pip install --force-reinstall torch==1.2.0 torchvision==0.4.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UryqtbowF3Ht",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6aa89a0-4151-4f26-80d3-b25936f4609c"
      },
      "source": [
        "%pylab inline\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import torch.utils.data.dataloader as dataloader\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torchsummary import summary\n",
        "\n",
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if use_cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['test']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y3iu-CF92Je"
      },
      "source": [
        "# Preparing the Dataset\n",
        "\n",
        "The \n",
        "\n",
        "MNIST dataset is normalized to [0,1] range as per the explanation here: https://stackoverflow.com/questions/63746182/correct-way-of-normalizing-and-scaling-the-mnist-dataset\n",
        "\n",
        "The dataset has the input MNIST image, a onehot encoding of a random number generated per test case, and the output which consists of the concatenation of the MNIST number (onehot) and a binary (decimal to binary) encoding of the sum. As the sum can vary from 0 to 19, 5 binary digits are needed, the dimension of the model output is hence 10+5 = 15 nodes. The target for MNIST is the MNIST label (instead of the one host encoding) so it is appended to the 5 digit sum.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WtaIzffIfO2"
      },
      "source": [
        "class MNIST_add_dataset(Dataset):\n",
        "  def __init__(self, trainset=True):\n",
        "    self.trainset=trainset\n",
        "    self.mnist_set = MNIST('./data', train=self.trainset, download=True, \n",
        "                                transform=transforms.Compose([\n",
        "                                    transforms.ToTensor(), # ToTensor does min-max normalization.\n",
        "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                ]), )\n",
        "    # list of random integers for adding - part of training set\n",
        "    self.rand_int_num_list = [np.random.randint(0,10) for i in range(len(self.mnist_set))]\n",
        "    #print(\"Rand int \",self.rand_int_num_list[0])\n",
        "    # convert it to 1 hot encoding for use in training\n",
        "    self.num_onehot = np.identity(10)[self.rand_int_num_list]    \n",
        "    #print(\"Rand int one hot \",self.num_onehot[0])\n",
        "    # get the list of MNIST digits\n",
        "    self.digit_list = list(map(lambda x:[x[1]], self.mnist_set))\n",
        "    #print(\"MNIST digit \",self.digit_list[0], len(self.digit_list))\n",
        "    # get the final train target label by summing the MNIST label and the random number\n",
        "    # and get the binary (5 digit) representation of the sum of the MNIST digit and the random number\n",
        "    self.bin_sum_list = list(map(lambda x,y: list(map(int,f'{x[1]+y:05b}')), self.mnist_set, self.rand_int_num_list))\n",
        "    #print(\"Binary of sum \",self.bin_sum_list[0])\n",
        "    # set the target as a concatenation of the MNIST label and the binary encoding of the sum of the \n",
        "    # MNIST number and the random number\n",
        "    self.target = list(map(lambda x,y:np.concatenate((x,y)),self.digit_list,self.bin_sum_list))\n",
        "    #print(\"Target \",self.target[0])\n",
        "  def __getitem__(self, index):\n",
        "    # MNIST image input\n",
        "    image = self.mnist_set[index][0]\n",
        "    # One hot encoding of the random number\n",
        "    oh_num = torch.as_tensor(self.num_onehot[index],dtype=torch.float32)\n",
        "    # concatenated target\n",
        "    target = torch.tensor(self.target[index])\n",
        "    return ([image, oh_num],target)        \n",
        "  def __len__(self):\n",
        "    return len(self.mnist_set)                          "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3IES_ojSjFM"
      },
      "source": [
        "# Make the train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hdAnxPCS1OI",
        "outputId": "a7c94b60-862c-4ed5-bd58-b80a7d9f6bcb"
      },
      "source": [
        "train_set = MNIST_add_dataset(trainset=True)\n",
        "test_set = MNIST_add_dataset(trainset=False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PomCjoo_SZM-"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxp7oMs5SdAg"
      },
      "source": [
        "# Data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnhnWfOuSfxe",
        "outputId": "060e527e-7b4d-4210-bb3b-4c58dd267c21"
      },
      "source": [
        "dataloader_args = dict(shuffle=True, batch_size=256,num_workers=4, pin_memory=True) if use_cuda else dict(shuffle=True, batch_size=64)\n",
        "train_loader = dataloader.DataLoader(train_set, **dataloader_args)\n",
        "test_loader = dataloader.DataLoader(test_set, **dataloader_args)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07_gZin_F3IL"
      },
      "source": [
        "## Model\n",
        "\n",
        "The model consists of 3 convolutional blocks, followed by 3 linear blocks, with ReLU activation in between. The MNIST image is fed to the 1st Conv block, and the ranbdom number (one hot) to the first linear block (alongwith the output of the 3rd Conv block, concatenated)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELkaVi6oUyIl"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    # conv layer 1\n",
        "    self.conv1 = nn.Sequential(\n",
        "                          \n",
        "        nn.Conv2d(1,16,5), # 16x24x24\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2)  # 16x12x12\n",
        "    )\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.Conv2d(16,32,5), # 32x8x8\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2)  # 32x4x4\n",
        "    )\n",
        "    self.conv3 = nn.Sequential(\n",
        "        nn.Conv2d(32,10,3), # 10x2x2\n",
        "        nn.MaxPool2d(2,2)   # 10x1x1\n",
        "    )\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc1 = nn.Linear(10+10, 60) # adding random number one hot to the 1x10 MNIST output\n",
        "    self.fc2 = nn.Linear(60, 30)\n",
        "    self.fc3 = nn.Linear(30, 15) # 10 for MNIST 1-hot coding, and 5 for binary repr of sum of digits\n",
        "\n",
        "  def forward(self, image, number):\n",
        "    #print(\"0 \",image.shape)\n",
        "    x = self.conv1(image)\n",
        "    #print(\"1 \",x.shape)\n",
        "    x = self.conv2(x)\n",
        "    #print(\"2 \",x.shape)\n",
        "    x = self.conv3(x)\n",
        "    #print(\"3 \",x.shape)\n",
        "    x = x.view(-1,10)\n",
        "    #print(\"after \",x.shape)\n",
        "    # concatenate the number\n",
        "    x = torch.cat((x,number),1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    x = x.view(-1,15)\n",
        "    #print(\"In forward x shape \",x.shape)\n",
        "    \n",
        "    # The first 10 outputs should be the onehot encoding of the MNIST digit\n",
        "    # using a Log softmax (with NLL Loss) for this\n",
        "    o1 = F.log_softmax(x[:,:10])\n",
        "    #print(\"In forward o1 shape \",o1.shape)\n",
        "\n",
        "    # for the 5 digit sum outout - as it is a multi-label classification, I am using a Sigmoid and not a softmax as there\n",
        "    # will be multiple 1's in the output\n",
        "    # used Hardsigmoid as it has a more sharp curve\n",
        "    sig = nn.Hardsigmoid()\n",
        "    o2 = sig(x[:,10:])\n",
        "    #print(\"In forward o2 shape \",o2.shape)\n",
        "    return torch.cat((o1,o2),1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b4-zt2wgaEQ",
        "outputId": "786cbaf1-74ad-4aed-993d-2ff7b91ee540"
      },
      "source": [
        "model = Model().to(device)\n",
        "print(model)\n",
        "# random test\n",
        "model.forward(torch.rand((1, 1, 28, 28)).to(device), torch.rand((1, 10)).to(device))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(32, 10, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (relu): ReLU()\n",
            "  (fc1): Linear(in_features=20, out_features=60, bias=True)\n",
            "  (fc2): Linear(in_features=60, out_features=30, bias=True)\n",
            "  (fc3): Linear(in_features=30, out_features=15, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.1280, -2.3497, -2.3076, -2.2818, -2.1473, -2.3670, -2.3914, -2.4043,\n",
              "         -2.3156, -2.3768,  0.5047,  0.4931,  0.4800,  0.4763,  0.4750]],\n",
              "       device='cuda:0', grad_fn=<CatBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KpjqMQBhJfv"
      },
      "source": [
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, losses):\n",
        "    print(f\"EPOCH - {epoch}\")\n",
        "    model.train()\n",
        "    for batch_idx, (input, target) in enumerate(train_loader):\n",
        "        image, number, target = input[0].to(device), input[1].to(device), target.to(device)\n",
        "        # clear the grad computation\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(image, number)  # Passing batch\n",
        "        #print(\"Input shape \", input.shape)\n",
        "        #print(len(image))\n",
        "        #print(\"Image shape \", image.shape)\n",
        "        #print(\"Number shape \", number.shape)\n",
        "        #print(\"Target shape \",target.shape)\n",
        "        #print(\"Ypred shape \",y_pred.shape)\n",
        "        \n",
        "        # Calculate loss\n",
        "        #print(target[:,0].shape)\n",
        "        #print(y_pred[:,:10])\n",
        "        # using 2 losses - one for the MNIST prediction and one for the sum (binary)\n",
        "        # using Negative log likelihood for the MNIST prediction as we used Log Softmax for the activation\n",
        "        loss_nll = nn.NLLLoss()\n",
        "        loss1 = loss_nll(y_pred[:,:10],target[:,0])\n",
        "        \n",
        "        \n",
        "        # Using Binary cross entropy for the binary sum representation\n",
        "        loss_bce = torch.nn.BCELoss()\n",
        "        loss2 = loss_bce(y_pred[:,10:].float(),target[:,1:].float())\n",
        "\n",
        "        # Total loss\n",
        "        loss=loss1+loss2\n",
        "        #print(\"Loss1 \",loss1.cpu().data.item())\n",
        "        #print(\"Loss2 \",loss2.cpu().data.item())\n",
        "        #print(\"Loss \",loss.cpu().data.item())\n",
        "        losses.append(loss.cpu().data.item())\n",
        "        \n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Display\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('\\r Train Epoch: {}/{} \\\n",
        "            [{}/{} ({:.0f}%)]\\\n",
        "            \\tAvg Loss: {:.6f}'.format(\n",
        "                epoch+1,\n",
        "                EPOCHS,\n",
        "                batch_idx * len(image), \n",
        "                len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), \n",
        "                loss.cpu().data.item()/256), \n",
        "                end='')\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct_MNIST = 0\n",
        "    correct_sum = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad(): # dont compute gradients \n",
        "        for data, target in test_loader:\n",
        "            image, number, target = data[0].to(device), data[1].to(device), target.to(device)\n",
        "            # get prediction\n",
        "            output = model(image, number)\n",
        "            #print(output.shape, target.shape)\n",
        "            #print(\"Output \",output,\"\\nTarget \", target)\n",
        "            \n",
        "            # compute loss\n",
        "            loss_nll = nn.NLLLoss()\n",
        "            loss1 = loss_nll(output[:,:10],target[:,0]).item()\n",
        "            \n",
        "            loss_bce = torch.nn.BCELoss()\n",
        "            loss2 = loss_bce(output[:,10:].float(),target[:,1:].float()).item()\n",
        "            \n",
        "            loss = loss1+loss2\n",
        "            \n",
        "            #print(\"Loss1 \",loss1.cpu().data.item())\n",
        "            #print(\"Loss2 \",loss2.cpu().data.item())\n",
        "            #print(\"Loss \",loss.cpu().data.item())\n",
        "\n",
        "            test_loss += loss  # sum up batch loss\n",
        "\n",
        "            #pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            #correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "            pred_MNIST = output[:,:10].argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct_MNIST_list = pred_MNIST.eq(target[:,0].view_as(pred_MNIST))\n",
        "            correct_MNIST += correct_MNIST_list.sum().item()\n",
        "            pred_sum = output[:,10:]\n",
        "            correct_sum_list = pred_sum.eq(target[:,1:].view_as(pred_sum))\n",
        "            correct_sum += correct_sum_list.sum().item()\n",
        "            correct_list = torch.logical_and(correct_MNIST_list, correct_sum_list)\n",
        "            correct += correct_list.sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, \\\n",
        "        Accuracy-MNIST: {}/{} ({:.0f}%)\\t\\\n",
        "        Accuracy-sum: {}/{} ({:.0f}%)\\t\\\n",
        "        Accuracy-total: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, \n",
        "        correct_MNIST, len(test_loader.dataset),\n",
        "        100. * correct_MNIST / len(test_loader.dataset),\n",
        "        correct_sum, len(test_loader.dataset),\n",
        "        100. * correct_sum / len(test_loader.dataset),\n",
        "        correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4EPlXfr7SY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "959de2cc-b35d-40f9-9519-b91e55f6dbbe"
      },
      "source": [
        "model = Model().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "EPOCHS=20\n",
        "losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train(model, device, train_loader, optimizer, epoch, losses)\n",
        "    test(model, device, test_loader)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH - 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Train Epoch: 1/20             [51200/60000 (85%)]            \tAvg Loss: 0.004698\n",
            "Test set: Average loss: 0.0042,         Accuracy-MNIST: 9073/10000 (91%)\t        Accuracy-sum: 4749/10000 (47%)\t        Accuracy-total: 4547/10000 (45%)\n",
            "\n",
            "EPOCH - 1\n",
            " Train Epoch: 2/20             [51200/60000 (85%)]            \tAvg Loss: 0.002582\n",
            "Test set: Average loss: 0.0030,         Accuracy-MNIST: 9634/10000 (96%)\t        Accuracy-sum: 6459/10000 (65%)\t        Accuracy-total: 6270/10000 (63%)\n",
            "\n",
            "EPOCH - 2\n",
            " Train Epoch: 3/20             [51200/60000 (85%)]            \tAvg Loss: 0.002450\n",
            "Test set: Average loss: 0.0029,         Accuracy-MNIST: 9731/10000 (97%)\t        Accuracy-sum: 7042/10000 (70%)\t        Accuracy-total: 6902/10000 (69%)\n",
            "\n",
            "EPOCH - 3\n",
            " Train Epoch: 4/20             [51200/60000 (85%)]            \tAvg Loss: 0.002783\n",
            "Test set: Average loss: 0.0030,         Accuracy-MNIST: 9786/10000 (98%)\t        Accuracy-sum: 7285/10000 (73%)\t        Accuracy-total: 7165/10000 (72%)\n",
            "\n",
            "EPOCH - 4\n",
            " Train Epoch: 5/20             [51200/60000 (85%)]            \tAvg Loss: 0.002326\n",
            "Test set: Average loss: 0.0025,         Accuracy-MNIST: 9818/10000 (98%)\t        Accuracy-sum: 6581/10000 (66%)\t        Accuracy-total: 6509/10000 (65%)\n",
            "\n",
            "EPOCH - 5\n",
            " Train Epoch: 6/20             [51200/60000 (85%)]            \tAvg Loss: 0.002612\n",
            "Test set: Average loss: 0.0029,         Accuracy-MNIST: 9733/10000 (97%)\t        Accuracy-sum: 7016/10000 (70%)\t        Accuracy-total: 6888/10000 (69%)\n",
            "\n",
            "EPOCH - 6\n",
            " Train Epoch: 7/20             [51200/60000 (85%)]            \tAvg Loss: 0.002438\n",
            "Test set: Average loss: 0.0025,         Accuracy-MNIST: 9861/10000 (99%)\t        Accuracy-sum: 6949/10000 (69%)\t        Accuracy-total: 6876/10000 (69%)\n",
            "\n",
            "EPOCH - 7\n",
            " Train Epoch: 8/20             [51200/60000 (85%)]            \tAvg Loss: 0.002384\n",
            "Test set: Average loss: 0.0026,         Accuracy-MNIST: 9855/10000 (99%)\t        Accuracy-sum: 7103/10000 (71%)\t        Accuracy-total: 7016/10000 (70%)\n",
            "\n",
            "EPOCH - 8\n",
            " Train Epoch: 9/20             [51200/60000 (85%)]            \tAvg Loss: 0.002491\n",
            "Test set: Average loss: 0.0024,         Accuracy-MNIST: 9832/10000 (98%)\t        Accuracy-sum: 6726/10000 (67%)\t        Accuracy-total: 6643/10000 (66%)\n",
            "\n",
            "EPOCH - 9\n",
            " Train Epoch: 10/20             [51200/60000 (85%)]            \tAvg Loss: 0.002388\n",
            "Test set: Average loss: 0.0024,         Accuracy-MNIST: 9854/10000 (99%)\t        Accuracy-sum: 6918/10000 (69%)\t        Accuracy-total: 6844/10000 (68%)\n",
            "\n",
            "EPOCH - 10\n",
            " Train Epoch: 11/20             [51200/60000 (85%)]            \tAvg Loss: 0.002255\n",
            "Test set: Average loss: 0.0024,         Accuracy-MNIST: 9876/10000 (99%)\t        Accuracy-sum: 7103/10000 (71%)\t        Accuracy-total: 7025/10000 (70%)\n",
            "\n",
            "EPOCH - 11\n",
            " Train Epoch: 12/20             [51200/60000 (85%)]            \tAvg Loss: 0.002285\n",
            "Test set: Average loss: 0.0024,         Accuracy-MNIST: 9843/10000 (98%)\t        Accuracy-sum: 7621/10000 (76%)\t        Accuracy-total: 7521/10000 (75%)\n",
            "\n",
            "EPOCH - 12\n",
            " Train Epoch: 13/20             [51200/60000 (85%)]            \tAvg Loss: 0.002190\n",
            "Test set: Average loss: 0.0023,         Accuracy-MNIST: 9865/10000 (99%)\t        Accuracy-sum: 7531/10000 (75%)\t        Accuracy-total: 7447/10000 (74%)\n",
            "\n",
            "EPOCH - 13\n",
            " Train Epoch: 14/20             [51200/60000 (85%)]            \tAvg Loss: 0.002547\n",
            "Test set: Average loss: 0.0023,         Accuracy-MNIST: 9859/10000 (99%)\t        Accuracy-sum: 8128/10000 (81%)\t        Accuracy-total: 8043/10000 (80%)\n",
            "\n",
            "EPOCH - 14\n",
            " Train Epoch: 15/20             [51200/60000 (85%)]            \tAvg Loss: 0.006193\n",
            "Test set: Average loss: 0.0044,         Accuracy-MNIST: 9394/10000 (94%)\t        Accuracy-sum: 6691/10000 (67%)\t        Accuracy-total: 6360/10000 (64%)\n",
            "\n",
            "EPOCH - 15\n",
            " Train Epoch: 16/20             [51200/60000 (85%)]            \tAvg Loss: 0.002497\n",
            "Test set: Average loss: 0.0026,         Accuracy-MNIST: 9769/10000 (98%)\t        Accuracy-sum: 6831/10000 (68%)\t        Accuracy-total: 6692/10000 (67%)\n",
            "\n",
            "EPOCH - 16\n",
            " Train Epoch: 17/20             [51200/60000 (85%)]            \tAvg Loss: 0.002771\n",
            "Test set: Average loss: 0.0025,         Accuracy-MNIST: 9819/10000 (98%)\t        Accuracy-sum: 7544/10000 (75%)\t        Accuracy-total: 7412/10000 (74%)\n",
            "\n",
            "EPOCH - 17\n",
            " Train Epoch: 18/20             [51200/60000 (85%)]            \tAvg Loss: 0.002303\n",
            "Test set: Average loss: 0.0024,         Accuracy-MNIST: 9826/10000 (98%)\t        Accuracy-sum: 8088/10000 (81%)\t        Accuracy-total: 7956/10000 (80%)\n",
            "\n",
            "EPOCH - 18\n",
            " Train Epoch: 19/20             [51200/60000 (85%)]            \tAvg Loss: 0.002461\n",
            "Test set: Average loss: 0.0023,         Accuracy-MNIST: 9862/10000 (99%)\t        Accuracy-sum: 8442/10000 (84%)\t        Accuracy-total: 8338/10000 (83%)\n",
            "\n",
            "EPOCH - 19\n",
            " Train Epoch: 20/20             [51200/60000 (85%)]            \tAvg Loss: 0.002242\n",
            "Test set: Average loss: 0.0022,         Accuracy-MNIST: 9845/10000 (98%)\t        Accuracy-sum: 9218/10000 (92%)\t        Accuracy-total: 9093/10000 (91%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "UQ2WLm9XFlY5",
        "outputId": "337fd376-4d6c-46f6-c822-578af48c7798"
      },
      "source": [
        "plot(losses)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fb0e489a990>]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6MIPulGF3It"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW-RbqX5F3It"
      },
      "source": [
        "evaluate_x = Variable(test_loader.dataset.data.type_as(torch.FloatTensor()))\n",
        "evaluate_y = Variable(test_loader.dataset.targets)\n",
        "if cuda:\n",
        "    evaluate_x, evaluate_y = evaluate_x.cuda(), evaluate_y.cuda()\n",
        "\n",
        "model.eval()\n",
        "output = model(evaluate_x)\n",
        "pred = output.data.max(1)[1]\n",
        "d = pred.eq(evaluate_y.data).cpu()\n",
        "accuracy = d.sum().item()/d.size().numel()\n",
        "\n",
        "print('Accuracy:', accuracy*100.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn7xNEYxzqwB"
      },
      "source": [
        "npi = evaluate_x.cpu().detach().numpy()\n",
        "npo = output.cpu().detach().numpy()\n",
        "for index in range(1):\n",
        "  print(npi[index])\n",
        "  print(npo[index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLSGOG-Vz5CN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxuZSmx2LjbK"
      },
      "source": [
        "dummy_input = torch.randn(784, device='cuda')\n",
        "torch.onnx.export(model, (dummy_input), \"mnist.onnx\", verbose=True)\n",
        "#                  input_names=input_names, output_names=output_names)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYSqvJE77p8S"
      },
      "source": [
        "pip install onnx==1.5.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80r-Xl5M7xnT"
      },
      "source": [
        "import onnx\n",
        "model=onnx.load_model('mnist.onnx')\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}